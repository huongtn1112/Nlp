{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "19021297_TranNgocHuong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huongtn1112/Nlp/blob/main/19021297_TranNgocHuong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPoBDzV5V8Ec"
      },
      "source": [
        "## Introduction\n",
        "In this assignment, you will learn how to build a language model from scratch and use the model to generate new text.\n",
        "You will also see how training a language model helps you learn word representation.\n",
        "\n",
        "Note: \n",
        "- Plagiarism will result in 0 mark.\n",
        "- The following template shows how your code should look like. You are free to add more functions, change the parameters. You are not allowed to use existing implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfBnJo6EV8Ei"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as ag\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s1EmmALV8Ek"
      },
      "source": [
        "## Recurrent Neural Network (5 points)\n",
        "To begin, you have to implement the vanila RNN in Pytorch.\n",
        "\n",
        "Recall the formula for vanila RNN:\n",
        "        \\begin{eqnarray}\n",
        "        h_t & = & \\sigma(W_h h_{t-1} + W_x x_t + b_1) \\\\\n",
        "        y_t & = & \\phi(W_y h_t + b_2)\n",
        "        \\end{eqnarray}\n",
        "where $\\sigma$ is the usually the sigmoid activation function and $\\phi$ is usually the softmax function.\n",
        "\n",
        "Hints:\n",
        "For RNNLM, the input is a squence of word_id, e.g. [10, 8, 5, 2, 101, 23]. You have to convert each word_id to an embedding vector. To implement this, you can use the `torch.nn.Embedding` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21QqwrxgV8El",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "ce79e771-10a0-45cf-e4ca-cc4c54730611"
      },
      "source": [
        "class VanilaRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a vanila RNN. \n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons\n",
        "        n_hiddens: number of hidden neurons\n",
        "        n_outputs: number of output neurons\n",
        "        vocab: a dictionary of the form {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.n_outputs = n_outputs\n",
        "        self.vocab = vocab\n",
        "        self.sigma = sigma\n",
        "        self.phi = phi\n",
        "      \n",
        "        self.w_h = nn.Parameter(torch.tensor(n_hiddens, n_hiddens), requires_grad=True)\n",
        "        self.w_x = nn.Parameter(torch.tensor(n_hidden, n_inputs), requires_grad=True)\n",
        "        self.w_b1 = nn.Parameter(torch.tensor(1, n_hiddens), requires_grad=True)\n",
        "        self.w_y = nn.Parameter(torch.tensor(n_outputs, n_hiddens), requires_grad=True)\n",
        "        self.w_b2 = nn.Parameter(torch.tensor(1, n_outputs), requires_grad=True)\n",
        "        self.embedding = nn.Embedding(n_outputs, embedding_dim, max_norm=True)\n",
        "\n",
        "    \n",
        "    def forward(self, xs, h0):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "        xs: the input sequence [x_1, x_2, ..., x_n]. x_i is the id of the i-th word in the sequence. \n",
        "            For example, xs = [1, 3, 11, 6, 8, 2]\n",
        "        h0: the initial hidden state\n",
        "        \n",
        "        Returns: (ys, hs) where\n",
        "        ys = [y_1, y_2, ..., y_n] and\n",
        "        hs = [h_1, h_2, ..., h_n]\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(1, n_hiddens)\n",
        "        h_t = torch.matmul(self.)\n",
        "        nn.LogSigmoid()\n",
        "        y_t = \n",
        "        pass\n",
        "embedding_dim = 10\n",
        "xs = [1, 2, 3]\n",
        "rnn = VanilaRNN()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-7bdba10053b2>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    h_t = torch.matmul(self.)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8rmOkXSV8Em"
      },
      "source": [
        "Vanila RNN suffers from the gradient vanishing/exploding problem. Your next task is to implement a more sophisticated RNN that is more robust to gradient vanishing/exploding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmABN-OV8Em"
      },
      "source": [
        "class FancyRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a fancy RNN, this could be LSTM, GRU, or your own invention.\n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons\n",
        "        n_hiddens: number of hidden neurons\n",
        "        n_outputs: number of output neurons\n",
        "        vocab: a dictionary {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.n_outputs = n_outputs\n",
        "        self.vocab = vocab\n",
        "        self.sigma = sigma\n",
        "        self.phi = phi\n",
        "\n",
        "    \n",
        "    def forward(self, xs, h0):\n",
        "        pass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgaOmUIdV8Em"
      },
      "source": [
        "## Language Modeling with RNN (4 points)\n",
        "The next step is to use our RNNs in some real world tasks. One of the most common application of RNN is language modeling.\n",
        "\n",
        "### Data\n",
        "For this assignment, we will use text data from Wikipedia. To start, download the data from this website:\n",
        "\n",
        "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
        "\n",
        "Some information about this dataset can be found here:\n",
        "\n",
        "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcaF05EjV8En"
      },
      "source": [
        "### Training a LM with RNN\n",
        "Write the code to train RNNLMs with the VanilaRNNLM and FancyRNNLM classes above. Train 1 instance of VanilaRNNLM and 1 instance of FancyRNNLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5RaKX26V8En"
      },
      "source": [
        "def train_rnnlm(corpus, rnnlm, **train_params):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    corpus: the text corpus\n",
        "    rnnlm: the RNN\n",
        "    train_params: other parameters, e.g. learning rate, batch size, number of GPUs, ...\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    return rnnlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj6OXCeBV8Eo"
      },
      "source": [
        "# vanila_rnnlm = train_rnnlm(corpus=wiki_corpus_train, rnnlm=vanila_rnn)\n",
        "# fancy_rnnlm = train_rnnlm(corpus=wiki_corpus_train, rnnlm=fancy_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkF1h9JHV8Eo"
      },
      "source": [
        "### Generating new text with RNNLM\n",
        "Write the code to generate new text segments from the RNNLM. Produce several outputs from both VanilaRNN and FancyRNN to compare the quality of 2 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh8csqVQV8Ep"
      },
      "source": [
        "def generate_text(rnnlm, seed_text, length, **params):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    rnnlm: the language model\n",
        "    seed_text: a string of initial text\n",
        "    length: the length of the generated text\n",
        "    params: other params\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29W4HFCPV8Ep"
      },
      "source": [
        "# seed_text = input(\"Enter your initial text\")\n",
        "# output_text = generate_text(rnnlm=rnnlm, seed_text=seed_text, length=100)\n",
        "# print(output_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYpghnFgV8Ep"
      },
      "source": [
        "### Perplexity (+2 bonus points)\n",
        "Compute the perplexity of the models. The lower the perplexity, the higher your score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdTJG4i5V8Ep"
      },
      "source": [
        "def perplexity(rnnlm, corpus):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_FqqOwV8Eq"
      },
      "source": [
        "# perp = perplexity(rnnlm=rnnlm, corpus=wiki_corpus_test)\n",
        "# print(perp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB28K5rHV8Eq"
      },
      "source": [
        "## Word Embedding (1 point + 1 bonus point)\n",
        "\n",
        "Now you have trained your RNNLM, the `torch.nn.Embedding` layer in your model stores the embeddings of words in the dictionary. You can use dimensionality reduction algorithms such as PCA and TSNE to visualize the word embeddings.\n",
        "Produce a 2D plot of 100 to 1000 words and write a short analysis of the plot (e.g. the clusters of words with similar meaning, arithmetic operations you can apply on these words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7AWMlzmV8Eq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}